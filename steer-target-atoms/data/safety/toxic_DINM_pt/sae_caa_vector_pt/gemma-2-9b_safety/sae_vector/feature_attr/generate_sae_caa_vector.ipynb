{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/xzwnlp/anaconda3/envs/sae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import plotly.express as px\n",
    "from typing import Any, Dict, Optional, Protocol, Tuple\n",
    "import os, sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sae_lens import SAE\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sae_lens.toolkit.pretrained_sae_loaders import (\n",
    "    gemma_2_sae_loader,\n",
    "    get_gemma_2_config,\n",
    ")\n",
    "from sae_lens import SAE, SAEConfig, LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "def load_gemma_2_sae(\n",
    "    sae_path: str,\n",
    "    device: str = \"cpu\",\n",
    "    repo_id: str = \"gemma-scope-9b-it-res\",\n",
    "    force_download: bool = False,\n",
    "    cfg_overrides: Optional[Dict[str, Any]] = None,\n",
    "    d_sae_override: Optional[int] = None,\n",
    "    layer_override: Optional[int] = None,\n",
    ") -> Tuple[Dict[str, Any], Dict[str, torch.Tensor], Optional[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Custom loader for Gemma 2 SAEs.\n",
    "    \"\"\"\n",
    "    cfg_dict = get_gemma_2_config(repo_id, sae_path, d_sae_override, layer_override)\n",
    "    cfg_dict[\"device\"] = device\n",
    "\n",
    "    # Apply overrides if provided\n",
    "    if cfg_overrides is not None:\n",
    "        cfg_dict.update(cfg_overrides)\n",
    "\n",
    "    # Load and convert the weights\n",
    "    state_dict = {}\n",
    "    with np.load(os.path.join(sae_path, \"params.npz\")) as data:\n",
    "        for key in data.keys():\n",
    "            state_dict_key = \"W_\" + key[2:] if key.startswith(\"w_\") else key\n",
    "            state_dict[state_dict_key] = (\n",
    "                torch.tensor(data[key]).to(dtype=torch.float32).to(device)\n",
    "            )\n",
    "\n",
    "    # Handle scaling factor\n",
    "    if \"scaling_factor\" in state_dict:\n",
    "        if torch.allclose(\n",
    "            state_dict[\"scaling_factor\"], torch.ones_like(state_dict[\"scaling_factor\"])\n",
    "        ):\n",
    "            del state_dict[\"scaling_factor\"]\n",
    "            cfg_dict[\"finetuning_scaling_factor\"] = False\n",
    "        else:\n",
    "            assert cfg_dict[\n",
    "                \"finetuning_scaling_factor\"\n",
    "            ], \"Scaling factor is present but finetuning_scaling_factor is False.\"\n",
    "            state_dict[\"finetuning_scaling_factor\"] = state_dict.pop(\"scaling_factor\")\n",
    "    else:\n",
    "        cfg_dict[\"finetuning_scaling_factor\"] = False\n",
    "\n",
    "    sae_cfg = SAEConfig.from_dict(cfg_dict)\n",
    "    sae = SAE(sae_cfg)\n",
    "    sae.load_state_dict(state_dict)\n",
    "\n",
    "    # No sparsity tensor for Gemma 2 SAEs\n",
    "    log_sparsity = None\n",
    "\n",
    "    return sae, log_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 24\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_131\", device=\"cpu\") # 15\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_73\", device=\"cpu\") # 17\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_132\", device=\"cpu\") # 19\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_129\", device=\"cpu\") # 21\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_123\", device=\"cpu\") # 22\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_120\", device=\"cpu\") # 23\n",
    "sae, sparsity = load_gemma_2_sae(f\"/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114\", device=\"cpu\") # 24\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_114\", device=\"cpu\") # 25\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_118\", device=\"cpu\") # 27\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_119\", device=\"cpu\") # 29\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_114\", device=\"cpu\") # 31\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_114\", device=\"cpu\") # 33\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_120\", device=\"cpu\") # 35\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_124\", device=\"cpu\") # 37\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_131\", device=\"cpu\") # 39\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_113\", device=\"cpu\") # 41\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_75\", device=\"cpu\") # 16\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_71\", device=\"cpu\") # 18\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_68\", device=\"cpu\") # 20\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_116\", device=\"cpu\") # 26\n",
    "# sae, sparsity = load_gemma_2_sae(f\"/mnt/16t/xzwnlp/hugging_cache/gemma-scope-9b-pt-res/layer_{layer}/width_16k/average_l0_119\", device=\"cpu\") # 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "neg_attr_name = f\"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_pt/sae_caa_vector_pt/gemma-2-9b_safety/act_toxic_freq/feature_attr/gemma-2-9b_sae_layer24_resid_post_16k_neg_feature_freq.pt\"\n",
    "pos_attr_name = f\"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_pt/sae_caa_vector_pt/gemma-2-9b_safety/act_toxic_freq/feature_attr/gemma-2-9b_sae_layer24_resid_post_16k_pos_feature_freq.pt\"\n",
    "neg_data = torch.load(neg_attr_name)\n",
    "pos_data = torch.load(pos_attr_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050.,\n",
       "        4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050.,\n",
       "        4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050.,\n",
       "        4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050.,\n",
       "        4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4049.],\n",
       "       device='cuda:0'),\n",
       "indices=tensor([  451,  1071,  1405,  1834,  1843,  1927,  2133,  2171,  2233,  3228,\n",
       "         3614,  3751,  4261,  4570,  4581,  4646,  5286,  6578,  6695,  6932,\n",
       "         6949,  7618,  7794,  8017,  9227,  9548,  9705, 10310, 10494, 10537,\n",
       "        10968, 10982, 11214, 11769, 11955, 12399, 12715, 12806, 13551, 14031,\n",
       "        14802, 14984, 15177, 15280, 15476, 15763, 15794, 15866, 16162,   459],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data.topk(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([3725., 3631., 3620., 3609., 3558., 3511., 3483., 3478., 3472., 3426.,\n",
       "        3420., 3373., 3345., 3318., 3274., 3264., 3210., 3209., 3202., 3176.,\n",
       "        3170., 3167., 3166., 3145., 3126., 3103., 3086., 3079., 3071., 3055.,\n",
       "        3054., 3025., 2964., 2961., 2957., 2936., 2921., 2917., 2912., 2885.,\n",
       "        2874., 2873., 2864., 2853., 2851., 2850., 2842., 2841., 2840., 2839.],\n",
       "       device='cuda:0'),\n",
       "indices=tensor([ 6942, 10705, 16061,  9127,  8912,  4212,   752,  5803,  4082,  2402,\n",
       "          427,  4071, 11065,  4279,  5512, 14310,  6587,  7423,  5356,  5537,\n",
       "         9853, 15839,  6142, 15049, 12313,  4309,  3657,  9911, 15878,   683,\n",
       "        10900, 16326, 10335, 15836,  8193, 12172, 13078,  3922, 15760,  3466,\n",
       "         5054,  9458,  7443,  2128,  1866,  7350, 13289,  1310, 11015, 14107],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pos_data-neg_data).topk(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([2114., 2069.], device='cuda:0'),\n",
       "indices=tensor([12, 32], device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pos_data-neg_data)[pos_data.topk(50).indices].topk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4261, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data.topk(50).indices[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2114., device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data[4261]-neg_data[4261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5759, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_attr_name = f\"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_pt/sae_caa_vector_pt/gemma-2-9b_safety/act_toxic_freq/feature_attr/gemma-2-9b_sae_layer24_resid_post_16k_feature_score.pt\"\n",
    "\n",
    "act_data = torch.load(act_attr_name)\n",
    "act_data[4261]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活和频率取前pec的交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3584])\n",
      "tensor([ 0.0020,  0.0015,  0.0046,  ..., -0.0091, -0.0010, -0.0160],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor(1., grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.zeros_like(act_data)  # 初始化综合得分\n",
    "scores[4261] = 1\n",
    "scores = scores.to(sae.W_dec.device)\n",
    "result = scores @ sae.W_dec\n",
    "print(result.shape)\n",
    "print(result)\n",
    "print(torch.norm(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering_vector_path = \"/data2/xzwnlp/SaeEdit/shae_old/data/toxic_DINM/feature/act_toxic_freq_or_act/steering_vector\"\n",
    "steering_vector_path = \"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_pt/sae_caa_vector_pt/gemma-2-9b_safety/freq_top1/\"\n",
    "steering_vector_name = f\"gemma-2-9b_sae_layer{layer}_resid_post_16k_steering_vector_top1.pt\"\n",
    "steering_vector_full_path = os.path.join(steering_vector_path, steering_vector_name)\n",
    "torch.save(result, steering_vector_full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
