Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_23/width_16k/average_l0_120', data_name='toxic_DINM', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[23], hook_module='resid_post', trim=[0.35])
##########layer[23]###########
##########layer23###########
no1-torch.norm(caa_vector): 62.57244110107422
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_23/width_16k/average_l0_120
act_data.norm：36.883296966552734
Negative data:153347.3125 

top 10:tensor([4050., 4050., 4050., 4050., 4048., 4047., 4047., 4047., 4046., 4045.],
       device='cuda:0')
Positive data:155324.53125 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:36.883296966552734 

top 10:tensor([17.3598,  7.4161,  6.9475,  6.1892,  6.0585,  5.4424,  5.1424,  4.8037,
         4.4083,  4.3837], device='cuda:0')
mask: tensor(14519, device='cuda:0')
频率阈值: 0.10174797475337982
prune_mask: tensor(5737, device='cuda:0')
######### act and fre ########
阈值: 0.020673692226409912
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4447, device='cuda:0')
tensor(654.9232, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.5384,  0.6046, -0.2704,  ...,  0.4790,  0.6248, -0.4587],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(38.8317, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.020673692226409912
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.8911,  0.2929, -0.4629,  ...,  0.7838,  0.5036, -1.1285],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(51.9083, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.5333,  0.5998, -0.2908,  ...,  0.4784,  0.6315, -0.4587],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(38.9637, grad_fn=<LinalgVectorNormBackward0>)
