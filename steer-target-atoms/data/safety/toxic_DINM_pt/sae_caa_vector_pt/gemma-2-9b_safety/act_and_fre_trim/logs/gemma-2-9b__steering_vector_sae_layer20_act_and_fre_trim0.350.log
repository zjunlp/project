Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_20/width_16k/average_l0_68', data_name='toxic_DINM', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[20], hook_module='resid_post', trim=[0.35])
##########layer[20]###########
##########layer20###########
no1-torch.norm(caa_vector): 53.9066162109375
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_20/width_16k/average_l0_68
act_data.norm：27.67820930480957
Negative data:111886.9140625 

top 10:tensor([4050., 4050., 4048., 4047., 4046., 4041., 4041., 4037., 4036., 4035.],
       device='cuda:0')
Positive data:115956.8203125 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:27.67820930480957 

top 10:tensor([10.2754,  8.0289,  5.9604,  5.6309,  4.8199,  4.5020,  4.0191,  3.9938,
         3.4772,  3.4455], device='cuda:0')
mask: tensor(14326, device='cuda:0')
频率阈值: 0.0591987743973732
prune_mask: tensor(5736, device='cuda:0')
######### act and fre ########
阈值: 0.011645205318927765
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4677, device='cuda:0')
tensor(492.3555, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.2709,  0.1713,  0.1752,  ..., -0.0177,  1.0043, -0.3944],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(32.9168, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.011645205318927765
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.6011, -0.0784,  0.1249,  ..., -0.0449,  0.8336, -0.6120],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(40.3564, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.2687,  0.1749,  0.1641,  ..., -0.0171,  1.0096, -0.3983],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(32.9669, grad_fn=<LinalgVectorNormBackward0>)
