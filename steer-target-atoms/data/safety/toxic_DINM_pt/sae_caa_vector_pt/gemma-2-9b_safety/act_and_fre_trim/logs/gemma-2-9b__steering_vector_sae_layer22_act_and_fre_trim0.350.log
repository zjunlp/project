Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_22/width_16k/average_l0_123', data_name='toxic_DINM', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[22], hook_module='resid_post', trim=[0.35])
##########layer[22]###########
##########layer22###########
no1-torch.norm(caa_vector): 58.156578063964844
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_22/width_16k/average_l0_123
act_data.norm：32.95598220825195
Negative data:154080.09375 

top 10:tensor([4050., 4050., 4050., 4050., 4050., 4050., 4050., 4049., 4049., 4049.],
       device='cuda:0')
Positive data:153626.046875 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:32.95598220825195 

top 10:tensor([10.8449,  7.0758,  7.0499,  6.7933,  6.0130,  5.1976,  4.0539,  3.9666,
         3.7408,  3.7206], device='cuda:0')
mask: tensor(14687, device='cuda:0')
频率阈值: 0.09896373003721237
prune_mask: tensor(5735, device='cuda:0')
######### act and fre ########
阈值: 0.018576938658952713
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4492, device='cuda:0')
tensor(605.6128, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.3257,  0.2486, -0.4362,  ...,  0.4612,  0.5845, -0.7908],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(35.8283, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.018576938658952713
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.5172,  0.1419, -0.3483,  ...,  0.1093,  0.2627, -1.0640],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(48.1030, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.3140,  0.2439, -0.4521,  ...,  0.4547,  0.5814, -0.7895],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(35.9333, grad_fn=<LinalgVectorNormBackward0>)
