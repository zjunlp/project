Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_21/width_16k/average_l0_129', data_name='toxic_DINM', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[21], hook_module='resid_post', trim=[0.35])
##########layer[21]###########
##########layer21###########
no1-torch.norm(caa_vector): 57.44952392578125
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_21/width_16k/average_l0_129
act_data.norm：32.987247467041016
Negative data:156645.953125 

top 10:tensor([4050., 4050., 4050., 4050., 4050., 4050., 4050., 4049., 4049., 4049.],
       device='cuda:0')
Positive data:157537.421875 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:32.987247467041016 

top 10:tensor([8.8388, 7.8164, 6.9313, 5.2612, 4.9039, 4.8716, 4.5918, 4.4464, 4.3605,
        4.0526], device='cuda:0')
mask: tensor(14837, device='cuda:0')
频率阈值: 0.10279154777526855
prune_mask: tensor(5737, device='cuda:0')
######### act and fre ########
阈值: 0.019399430602788925
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4425, device='cuda:0')
tensor(577.1636, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.4990,  0.2896,  0.2912,  ..., -0.0164,  0.9594, -0.1170],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(34.2240, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.019399430602788925
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.8013,  0.2975,  0.2833,  ..., -0.0265,  0.8692, -0.4668],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(46.6279, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.4795,  0.2933,  0.2795,  ..., -0.0183,  0.9648, -0.1251],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(34.3176, grad_fn=<LinalgVectorNormBackward0>)
