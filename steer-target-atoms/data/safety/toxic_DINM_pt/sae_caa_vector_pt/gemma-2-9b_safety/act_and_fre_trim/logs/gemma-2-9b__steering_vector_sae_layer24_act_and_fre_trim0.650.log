Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114', data_name='toxic_DINM', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[24], hook_module='resid_post', trim=[0.65])
##########layer[24]###########
##########layer24###########
no1-torch.norm(caa_vector): 65.33668518066406
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114
act_data.norm：37.64331817626953
Negative data:151014.53125 

top 10:tensor([4050., 4050., 4050., 4050., 4050., 4049., 4049., 4049., 4049., 4048.],
       device='cuda:0')
Positive data:154833.71875 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:37.64331817626953 

top 10:tensor([15.1997,  9.2769,  8.1368,  7.4797,  7.1131,  5.0247,  4.7976,  4.7731,
         4.6402,  4.5904], device='cuda:0')
mask: tensor(14610, device='cuda:0')
频率阈值: 0.017181208357214928
prune_mask: tensor(10655, device='cuda:0')
######### act and fre ########
阈值: 0.0029935743659734726
act_top_mask: tensor(10650, device='cuda:0')
combined_mask: tensor(9638, device='cuda:0')
tensor(837.8927, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.5926,  0.1648, -0.5991,  ...,  0.4143,  1.0929, -0.9184],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(51.7154, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.0029935743659734726
act_top_mask: tensor(10650, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.6676,  0.2983, -0.4909,  ...,  0.1852,  0.8156, -0.9445],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(55.6262, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.5928,  0.1650, -0.5995,  ...,  0.4131,  1.0933, -0.9176],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(51.7207, grad_fn=<LinalgVectorNormBackward0>)
