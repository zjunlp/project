Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_27/width_16k/average_l0_118', data_name='toxic_DINM', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[27], hook_module='resid_post', trim=[0.65])
##########layer[27]###########
##########layer27###########
no1-torch.norm(caa_vector): 81.17648315429688
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_27/width_16k/average_l0_118
act_data.norm：46.900028228759766
Negative data:146203.046875 

top 10:tensor([4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050.],
       device='cuda:0')
Positive data:154391.484375 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:46.900028228759766 

top 10:tensor([15.5594, 15.1623,  8.9625,  8.1781,  6.5242,  6.4076,  6.0612,  5.5436,
         5.5269,  5.3453], device='cuda:0')
mask: tensor(14665, device='cuda:0')
频率阈值: 0.015163002535700798
prune_mask: tensor(10655, device='cuda:0')
######### act and fre ########
阈值: 0.003950633108615875
act_top_mask: tensor(10650, device='cuda:0')
combined_mask: tensor(9596, device='cuda:0')
tensor(1042.8051, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.9823, -0.1871, -0.9251,  ...,  0.6167,  1.9028, -1.3168],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(57.1837, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.003950633108615875
act_top_mask: tensor(10650, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.8061,  0.0515, -1.1619,  ...,  0.4229,  1.8074, -1.3561],
       grad_fn=<SqueezeBackward4>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.9811, -0.1852, -0.9241,  ...,  0.6151,  1.9044, -1.3141],
       grad_fn=<SqueezeBackward4>)
