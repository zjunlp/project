Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_26/width_16k/average_l0_116', data_name='toxic_DINM', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[26], hook_module='resid_post', trim=[0.35])
##########layer[26]###########
##########layer26###########
no1-torch.norm(caa_vector): 77.35136413574219
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_26/width_16k/average_l0_116
act_data.norm：41.5208854675293
Negative data:147897.28125 

top 10:tensor([4050., 4050., 4050., 4050., 4049., 4049., 4049., 4049., 4049., 4049.],
       device='cuda:0')
Positive data:155288.59375 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:41.5208854675293 

top 10:tensor([16.1910,  8.1207,  7.8204,  7.0348,  6.4624,  6.0185,  5.9149,  5.7151,
         5.4986,  5.2637], device='cuda:0')
mask: tensor(14563, device='cuda:0')
频率阈值: 0.10083580762147903
prune_mask: tensor(5735, device='cuda:0')
######### act and fre ########
阈值: 0.02666512131690979
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4455, device='cuda:0')
tensor(836.6609, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.6487,  0.3151, -0.5663,  ...,  0.3580,  0.8110, -0.7460],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(49.8549, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.02666512131690979
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.8163,  0.4559, -0.9156,  ...,  0.4871,  0.9624, -1.2068],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(63.6589, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.6635,  0.3280, -0.5747,  ...,  0.3412,  0.8111, -0.7394],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(50.0416, grad_fn=<LinalgVectorNormBackward0>)
