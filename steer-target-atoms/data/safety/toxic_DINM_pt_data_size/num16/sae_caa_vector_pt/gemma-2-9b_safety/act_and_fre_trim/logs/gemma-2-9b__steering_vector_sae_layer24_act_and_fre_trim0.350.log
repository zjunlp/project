Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_train', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114', data_name='num16', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[24], hook_module='resid_post', trim=[0.35])
##########layer[24]###########
##########layer24###########
no1-torch.norm(caa_vector): 65.52981567382812
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114
act_data.norm：42.45021057128906
Negative data:529.9877319335938 

top 10:tensor([16., 16., 16., 16., 16., 16., 16., 16., 16., 16.], device='cuda:0')
Positive data:633.7578125 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:42.45021057128906 

top 10:tensor([15.7392, 15.0402, 11.4189, 10.2115,  7.7013,  5.8079,  5.2196,  4.7294,
         4.6231,  4.3828], device='cuda:0')
mask: tensor(8559, device='cuda:0')
频率阈值: 0.125
prune_mask: tensor(5758, device='cuda:0')
######### act and fre ########
阈值: 0.022697534412145615
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4268, device='cuda:0')
tensor(824.5050, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.5357, -0.0248, -0.1671,  ...,  0.1715,  0.6478, -1.1127],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(56.1002, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.022697534412145615
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.8092,  0.4545,  0.1379,  ...,  0.2035,  0.4899, -1.3799],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(62.9488, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.5408, -0.0257, -0.1862,  ...,  0.1637,  0.6655, -1.1184],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(56.2013, grad_fn=<LinalgVectorNormBackward0>)
