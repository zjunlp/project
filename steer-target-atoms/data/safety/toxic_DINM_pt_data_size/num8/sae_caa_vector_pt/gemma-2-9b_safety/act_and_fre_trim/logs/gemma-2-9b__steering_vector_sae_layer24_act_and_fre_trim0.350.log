Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_train', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114', data_name='num8', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[24], hook_module='resid_post', trim=[0.35])
##########layer[24]###########
##########layer24###########
no1-torch.norm(caa_vector): 65.52981567382812
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114
act_data.norm：37.94925308227539
Negative data:304.8753967285156 

top 10:tensor([8., 8., 8., 8., 8., 8., 8., 8., 8., 8.], device='cuda:0')
Positive data:311.84771728515625 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:37.94925308227539 

top 10:tensor([12.6660, 11.9623, 10.9121, 10.5251,  6.6342,  5.8175,  5.6531,  4.3295,
         4.3262,  4.2924], device='cuda:0')
mask: tensor(7263, device='cuda:0')
频率阈值: 0.125
prune_mask: tensor(7263, device='cuda:0')
######### act and fre ########
阈值: 0.02449880912899971
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4607, device='cuda:0')
tensor(761.5712, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.2621, -0.0478, -0.0283,  ..., -0.0326,  0.8116, -0.8651],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(46.9009, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.02449880912899971
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.4250,  0.2198,  0.2227,  ..., -0.2671,  0.5673, -1.2690],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(54.3307, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.2649, -0.0371, -0.0375,  ..., -0.0524,  0.7929, -0.8948],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(47.0623, grad_fn=<LinalgVectorNormBackward0>)
