Namespace(path_dir='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_train', sae_path='/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114', data_name='num32', model_name='gemma-2-9b', mode='safety', select_type='act_and_fre_trim', layers=[24], hook_module='resid_post', trim=[0.35])
##########layer[24]###########
##########layer24###########
no1-torch.norm(caa_vector): 65.52981567382812
type(sae_path)
<class 'str'>

/data2/xzwnlp/gemma-scope-9b-pt-res/layer_24/width_16k/average_l0_114
act_data.norm：41.72061538696289
Negative data:1110.368896484375 

top 10:tensor([32., 32., 32., 32., 32., 32., 32., 32., 32., 32.], device='cuda:0')
Positive data:1246.322998046875 

top 10:torch.sort(pos_data, descending=True)[0][:10]
Activation data:41.72061538696289 

top 10:tensor([16.9928, 13.7876, 10.6028,  9.6052,  7.7915,  5.3332,  4.9371,  4.8187,
         4.7809,  4.4230], device='cuda:0')
mask: tensor(10153, device='cuda:0')
频率阈值: 0.09375
prune_mask: tensor(6447, device='cuda:0')
######### act and fre ########
阈值: 0.022903619334101677
act_top_mask: tensor(5735, device='cuda:0')
combined_mask: tensor(4533, device='cuda:0')
tensor(817.9143, device='cuda:0')
result_combined.shape torch.Size([3584])
result_combined: tensor([ 0.6691, -0.0199, -0.3745,  ...,  0.0821,  0.7623, -0.9868],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_combined) tensor(55.6306, grad_fn=<LinalgVectorNormBackward0>)
########### only act ########
阈值: 0.022903619334101677
act_top_mask: tensor(5735, device='cuda:0')
result_act.shape: torch.Size([3584])
result_act: tensor([ 0.8381,  0.3138, -0.2338,  ...,  0.1394,  0.6416, -1.1992],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_act): tensor(61.7128, grad_fn=<LinalgVectorNormBackward0>)
########### only fre ########
result_fre.shape: torch.Size([3584])
result_fre: tensor([ 0.6701, -0.0170, -0.4171,  ...,  0.0773,  0.7589, -1.0018],
       grad_fn=<SqueezeBackward4>)
torch.norm(result_fre): tensor(55.7806, grad_fn=<LinalgVectorNormBackward0>)
