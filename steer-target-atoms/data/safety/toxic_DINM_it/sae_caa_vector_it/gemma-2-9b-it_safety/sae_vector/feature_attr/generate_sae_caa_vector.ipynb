{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/xzwnlp/anaconda3/envs/sae/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import plotly.express as px\n",
    "from typing import Any, Dict, Optional, Protocol, Tuple\n",
    "import os, sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sae_lens import SAE\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sae_lens.toolkit.pretrained_sae_loaders import (\n",
    "    gemma_2_sae_loader,\n",
    "    get_gemma_2_config,\n",
    ")\n",
    "from sae_lens import SAE, SAEConfig, LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "\n",
    "def load_gemma_2_sae(\n",
    "    sae_path: str,\n",
    "    device: str = \"cpu\",\n",
    "    repo_id: str = \"gemma-scope-9b-it-res\",\n",
    "    force_download: bool = False,\n",
    "    cfg_overrides: Optional[Dict[str, Any]] = None,\n",
    "    d_sae_override: Optional[int] = None,\n",
    "    layer_override: Optional[int] = None,\n",
    ") -> Tuple[Dict[str, Any], Dict[str, torch.Tensor], Optional[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Custom loader for Gemma 2 SAEs.\n",
    "    \"\"\"\n",
    "    cfg_dict = get_gemma_2_config(repo_id, sae_path, d_sae_override, layer_override)\n",
    "    cfg_dict[\"device\"] = device\n",
    "\n",
    "    # Apply overrides if provided\n",
    "    if cfg_overrides is not None:\n",
    "        cfg_dict.update(cfg_overrides)\n",
    "\n",
    "    # Load and convert the weights\n",
    "    state_dict = {}\n",
    "    with np.load(os.path.join(sae_path, \"params.npz\")) as data:\n",
    "        for key in data.keys():\n",
    "            state_dict_key = \"W_\" + key[2:] if key.startswith(\"w_\") else key\n",
    "            state_dict[state_dict_key] = (\n",
    "                torch.tensor(data[key]).to(dtype=torch.float32).to(device)\n",
    "            )\n",
    "\n",
    "    # Handle scaling factor\n",
    "    if \"scaling_factor\" in state_dict:\n",
    "        if torch.allclose(\n",
    "            state_dict[\"scaling_factor\"], torch.ones_like(state_dict[\"scaling_factor\"])\n",
    "        ):\n",
    "            del state_dict[\"scaling_factor\"]\n",
    "            cfg_dict[\"finetuning_scaling_factor\"] = False\n",
    "        else:\n",
    "            assert cfg_dict[\n",
    "                \"finetuning_scaling_factor\"\n",
    "            ], \"Scaling factor is present but finetuning_scaling_factor is False.\"\n",
    "            state_dict[\"finetuning_scaling_factor\"] = state_dict.pop(\"scaling_factor\")\n",
    "    else:\n",
    "        cfg_dict[\"finetuning_scaling_factor\"] = False\n",
    "\n",
    "    sae_cfg = SAEConfig.from_dict(cfg_dict)\n",
    "    sae = SAE(sae_cfg)\n",
    "    sae.load_state_dict(state_dict)\n",
    "\n",
    "    # No sparsity tensor for Gemma 2 SAEs\n",
    "    log_sparsity = None\n",
    "\n",
    "    return sae, log_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 20\n",
    "sae, sparsity = load_gemma_2_sae(f\"/disk3/wmr/hugging_cache/gemma-scope-9b-it-res/layer_20/width_16k/average_l0_91\", device=\"cpu\") # 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "neg_attr_name = f\"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_it/sae_caa_vector_it/gemma-2-9b-it_safety/act_toxic_freq/feature_attr/gemma-2-9b-it_sae_layer20_resid_post_16k_neg_feature_freq.pt\"\n",
    "pos_attr_name = f\"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_it/sae_caa_vector_it/gemma-2-9b-it_safety/act_toxic_freq/feature_attr/gemma-2-9b-it_sae_layer20_resid_post_16k_pos_feature_freq.pt\"\n",
    "neg_data = torch.load(neg_attr_name)\n",
    "pos_data = torch.load(pos_attr_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050.,\n",
       "        4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050.,\n",
       "        4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4050., 4049.,\n",
       "        4049., 4049., 4049., 4049., 4048., 4048., 4048., 4048., 4048., 4048.,\n",
       "        4048., 4048., 4048., 4048., 4048., 4048., 4047., 4047., 4047., 4047.],\n",
       "       device='cuda:0'),\n",
       "indices=tensor([  771,  1295,  2226,  2698,  2962,  3200,  3206,  4480,  4851,  5045,\n",
       "         5183,  5299,  5677,  6880,  8522,  8736,  9015,  9071,  9436, 10145,\n",
       "        10964, 11395, 12023, 12789, 13937, 14203, 14878, 15913, 16043,  1107,\n",
       "         9755, 14195, 14688, 15718,  3219,  4147,  5956,  5999,  6588,  6670,\n",
       "         6794,  8384,  9462, 11927, 15294, 15655,  1415,  1694,  2065,  3972],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data.topk(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([3849., 3710., 3659., 3606., 3554., 3419., 3382., 3369., 3319., 3319.,\n",
       "        3291., 3280., 3276., 3273., 3183., 3139., 3135., 3087., 3029., 3016.,\n",
       "        2980., 2973., 2966., 2965., 2964., 2928., 2928., 2917., 2916., 2912.,\n",
       "        2905., 2894., 2890., 2887., 2883., 2861., 2842., 2841., 2831., 2828.,\n",
       "        2813., 2807., 2801., 2790., 2771., 2769., 2767., 2740., 2736., 2732.],\n",
       "       device='cuda:0'),\n",
       "indices=tensor([10457,  8228,  7276,  8664,  2834,  5566,  7854,  3791,  1883,  9015,\n",
       "          683, 15762,  7521,  1264, 16170, 11700, 13144,  5344, 12309,   484,\n",
       "         5168, 10940,  9128, 10737, 14055,  9272, 12957, 10731, 15956,  7104,\n",
       "         7779,  6791,  5469,  5191,  3206,  1396, 14455,  3032,  8340, 10166,\n",
       "         9918,  2112, 16069,  5964,  2446,  6968,  8780,  7780,  1527, 11198],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pos_data-neg_data).topk(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([3319., 2883.], device='cuda:0'),\n",
       "indices=tensor([16,  6], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pos_data-neg_data)[pos_data.topk(50).indices].topk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9015, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data.topk(50).indices[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3319., device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data[9015]-neg_data[9015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9905, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_attr_name = f\"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_it/sae_caa_vector_it/gemma-2-9b-it_safety/act_toxic_freq/feature_attr/gemma-2-9b-it_sae_layer20_resid_post_16k_feature_score.pt\"\n",
    "\n",
    "act_data = torch.load(act_attr_name)\n",
    "act_data[9015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活和频率取前pec的交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3584])\n",
      "tensor([ 0.0181, -0.0096,  0.0040,  ...,  0.0206,  0.0270,  0.0073],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor(1., grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.zeros_like(act_data)  # 初始化综合得分\n",
    "scores[9015] = 1\n",
    "scores = scores.to(sae.W_dec.device)\n",
    "result = scores @ sae.W_dec\n",
    "print(result.shape)\n",
    "print(result)\n",
    "print(torch.norm(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0181, -0.0096,  0.0040,  ...,  0.0206,  0.0270,  0.0073],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.W_dec[9015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering_vector_path = \"/data2/xzwnlp/SaeEdit/shae_old/data/toxic_DINM/feature/act_toxic_freq_or_act/steering_vector\"\n",
    "steering_vector_path = \"/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/toxic_DINM_it/sae_caa_vector_it/gemma-2-9b-it_safety/freq_top1/\"\n",
    "steering_vector_name = f\"gemma-2-9b-it_sae_layer{layer}_resid_post_16k_steering_vector_top1.pt\"\n",
    "steering_vector_full_path = os.path.join(steering_vector_path, steering_vector_name)\n",
    "torch.save(result, steering_vector_full_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
