WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.
Namespace(batch_size=1, select_type='only_sys', output_file='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/prompt_vector/only_sys_ax/sae_caa_vector_it/gemma-2-9b-it/only_sys/feature_only_sys_gemma-2-9b-it_layer20_16k_resid_post.json', data_file='/data2/xzwnlp/SaeEdit/ManipulateSAE/data/safety/prompt_vector/only_sys_ax/train.csv', data_name='only_sys_ax', model_name_or_path='/disk3/wmr/hugging_cache/gemma-2-9b-it', sae_path='/disk3/wmr/hugging_cache/gemma-scope-9b-it-res/layer_20/width_16k/average_l0_91', mode='safety', model_name='gemma-2-9b-it', system_prompt='', input_format_way='custom', steering_vector_name='gemma-2-9b-it_sae_layer20_resid_post_16k_steering_vector.pt', AB=False)
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.66it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  3.20it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  3.65it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.88it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.62it/s]
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
Loaded pretrained model /disk3/wmr/hugging_cache/gemma-2-9b-it into HookedTransformer
#########mode:safety#############
chose:   
Computing the activation selection for activation_selection_contrastive_for_prompt
act for safety:   0%|          | 0/1 [00:00<?, ?it/s]act for safety: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]act for safety: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]
ques_chosen_tokens:  tensor([[     2,    106,   1645,    108,   2045,    708,    671,  16481,  20409,
            674,  73711,   6758,   6514,    578,  38096,  11422, 235254,  37007,
          32520, 235269,  74771, 235269,    689,  24615,   3381,  11422, 235254,
          59519,  16961,    578,   2447,   3853,   3749,   5791,  11422, 235254,
         164287,  17120,    578, 105240,  11422, 235254,   9792,   7035,    578,
          33970,  17923,  11422, 235254,  40983,   4465,  25623,    578,   5656,
          67239,   8269,  11422, 235254,  34769,  32520,  16254,    577,   5985,
          25872,  11422, 235254,    107,    108,    106,   2516,    108]],
       device='cuda:0')
ques_rej_tokens:  tensor([[   2,  106, 1645,  108,  107,  108,  106, 2516,  108]],
       device='cuda:0')
pos_tokens:  tensor([[     2,    106,   1645,    108,   2045,    708,    671,  16481,  20409,
            674,  73711,   6758,   6514,    578,  38096,  11422, 235254,  37007,
          32520, 235269,  74771, 235269,    689,  24615,   3381,  11422, 235254,
          59519,  16961,    578,   2447,   3853,   3749,   5791,  11422, 235254,
         164287,  17120,    578, 105240,  11422, 235254,   9792,   7035,    578,
          33970,  17923,  11422, 235254,  40983,   4465,  25623,    578,   5656,
          67239,   8269,  11422, 235254,  34769,  32520,  16254,    577,   5985,
          25872,  11422, 235254,    107,    108,    106,   2516,    108, 235248]],
       device='cuda:0')
pos_feature_acts:  torch.Size([1, 1, 16384])
neg_tokens:  tensor([[     2,    106,   1645,    108,    107,    108,    106,   2516,    108,
         235248]], device='cuda:0')
neg_feature_acts:  torch.Size([1, 1, 16384])
feature_score:  torch.Size([16384])
feature_score:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
pos_feature_freq:  torch.Size([16384])
pos_feature_freq:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
neg_feature_freq:  torch.Size([16384])
neg_feature_freq:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
pos_act_mean:  torch.Size([16384])
pos_act_mean:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
neg_act_mean:  torch.Size([16384])
neg_act_mean:  tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
steering_vector.shape: torch.Size([3584])
steering_vector: tensor([-0.1715, -1.1925,  0.2157,  ..., -0.6642,  0.0257,  2.0604],
       device='cuda:0', grad_fn=<SqueezeBackward4>)
steering_vector.norm: tensor(106.4587, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)
feature_attr.shape: torch.Size([16384])
pos_feature_freq.shape: torch.Size([16384])
neg_feature_freq.shape: torch.Size([16384])
pos_act_mean.shape: torch.Size([16384])
neg_act_mean.shape: torch.Size([16384])
