<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Automatic Agent Learning from Scratch via Self-Planning">
  <meta name="keywords" content="AutoAct, Agent Learning, Self-Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoAct: Automatic Agent Learning from Scratch via Self-Planning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/meta.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
		/* Define the grid layout */
		.mygrid {
			display: grid;
			grid-template-columns: repeat(3, 1fr);
			grid-gap: 20px;
			width: 80%;
			margin: auto;
		}
		.grid_item {
      background: #FFFFFF;
      opacity: 1;
    }

		/* Define the size of the GIFs */
		.mygif {
			height: auto;
			cursor: pointer;
		}
		
		/* Define the modal styles */
		.modal {
			display: none;
			position: fixed;
			z-index: 1;
			left: 0;
			top: 0;
			width: 100%;
			height: 100%;
			overflow: auto;
			background-color: rgba(0,0,0,0.9);
		}
		
		.modal-content {
			margin: auto;
			display: block;
			width: 80%;
			max-width: 800px;
			max-height: 80%;
		}

    /* Define the full-screen overlay styles */
		.overlay {
			position: fixed;
			z-index: 999;
			left: 0;
			top: 0;
			width: 100%;
			height: 100%;
			overflow: hidden;
			background-color: rgba(0,0,0,0.9);
			display: none;
		}
		
		.overlay img {
			width: auto;
			height: 90%;
			margin: 0 auto;
			display: block;
			max-width: 90%;
			max-height: 90%;
		}

    /* Define the video styles */
		.gifvideo {
			width: 100%;
			height: auto;
		}

		/* Define the progress bar styles */
		.progress {
			width: 100%;
			height: 10px;
			background-color: #ddd;
			position: relative;
		}

		.progress-bar {
			height: 100%;
			background-color: #4CAF50;
			position: absolute;
			top: 0;
			left: 0;
		}
		
		/* Define the close button style */
		.close {
			color: white;
			position: absolute;
			top: 10px;
			right: 25px;
			font-size: 35px;
			font-weight: bold;
			cursor: pointer;
		}
		
		.close:hover,
		.close:focus {
			color: #bbb;
			text-decoration: none;
			cursor: pointer;
		}
	</style>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title" style="width: 110%; margin-left: -5%">AutoAct: Automatic Agent Learning from Scratch via Self-Planning</h2>
          <div class="is-size-5">
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Shuofei Qiao<sup>&#x2660;&#x2661;</sup>
            </span>, 
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Ningyu Zhang<sup>&#x2660;&#x2661;*</sup>
            </span>, 
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Runnan Fang<sup>&#x2660;&#x2661;</sup>
            </span>, 
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Yujie Luo<sup>&#x2660;&#x2661;</sup>
            </span>,
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Wangchunshu Zhou<sup>&#x2663;</sup>
            </span>,
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Yuchen Eleanor Jiang<sup>&#x2663;</sup>
            </span>,
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Chengfei Lv<sup>&#x2662;</sup>
            </span>,
            <span class="author-block" style="color:#00A4EF;font-weight:normal;">
              Huajun Chen<sup>&#x2660;&#x2661;*</sup>
            </span>,
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>&#x2660;</sup>Zhejiang University
            </span>
            <span class="author-block">
              <sup>&#x2661;</sup>Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph
            </span>
            <span class="author-block">
              <sup>&#x2663;</sup>AIWaves Inc.
            </span>
            <span class="author-block">
              <sup>&#x2662;</sup>Alibaba Group
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Corresponding Author</span>
           
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2401.05268" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zjunlp/AutoAct" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="100%" src="./images/first.gif">

      <h2 class="subtitle has-text-centered">
        Armed with just one tool library, the <b>Meta-Agent</b> can automatically differentiate based on the target task information and produce a sub-agent group that can collaborate to complete the task.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Language agents have achieved considerable performance on various complex tasks.
            Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions.
            To this end, we introduce <b>AutoAct</b>, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4).
            Given limited data with a tool library, <b>AutoAct</b> first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models.
            Then, <b>AutoAct</b> leverages a <i>division-of-labor</i> strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task.
            We conduct comprehensive experiments with different LLMs, which demonstrates that <b>AutoAct</b> yields better or parallel performance compared to various strong baselines.
            We even notice that <b>AutoAct</b>, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <br>
    <br>
    <!-- Paper Model. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">AutoAct</h2>
        <img id="model" width="100%" src="images/method.png">
        <p class="has-text-centered">
          Figure 1: <b>The overview of our proposed framework AutoAct</b>.
        </p>
        <br>
        <div class="column has-text-justified">
          As shown in Figure 1, AutoAct only requires target task information and a language agent (we name it <b>Meta-Agent</b>) to initiate its work.
          The Meta-Agent first augments the task data from scratch by self-instruct.
          Furthermore, with a tool library available, the Meta-Agent conducts automatic agent learning by differentiating into sub-agents with distinct functionalities and enabling them to perform group task-specific planning.
          We name this process as <b>self-planning</b>.
        </div>
      </div>
    </div>
    <br>
    <br>
    <!-- Paper Model. -->
    
    <!-- Paper Main Results -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Main Results</h2>
        <img id="model" width="80%" src="images/main_result.png">
        <p class="has-text-centered">
          Table 1: <b>Main results of AutoAct compared to various baselines on HotpotQA and ScienceQA.</b>
          The icon <i class="fas fa-toggle-off"></i> indicates prompt-based agent learning without fine-tuning, while <i class="fas fa-toggle-on"></i> means fine-tuning-based agent learning.
          <i class="fas fa-user"></i> denotes single-agent learning and <i class="fas fa-users"></i> symbolizes multi-agent learning.
          The best results of each model are marked in <b>bold</b> and the second-best results are marked with <u>underline</u>.
        </p>
        <br>
        <img id="model" width="40%" src="images/ablation.png">
        <p class="has-text-centered">
          Table 2: <b>Approach ablations of AutoAct.</b>
          <b><i>- reflection</i></b> symbolizes removing the reflect-agent in AutoAct.
          <b><i>- multi</i></b> denotes feeding all the differentiated data into one model for fine-tuning.
          <b><i>- fine-tuning</i></b> indicates zero-shot prompt planning with the three agents defined in AutoAct.
          <b><i>- filtering</i></b> represents self-differentiation on all the trajectories generated in zero-shot planning without filtering wrong cases.
        </p>
        <br>
      </div>
    </div>
    <br>
    <br>
    <!-- Paper Main Results -->

    <!-- Paper Analysis -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Analysis</h2>
        <img id="model" width="80%" src="images/ft_num.png">
        <p class="has-text-centered">
          Figure 2: <b>Performance of AutoAct on different training data scales.</b>
          (a-c) represents the results of the model trained on self-synthesized trajectories.
          (d-f) represents the results of the model trained on trajectories synthesized by a stronger model, where the dashed line is the baseline trained on self-synthesized trajectories.
        </p>
        <br>
        <img id="model" width="80%" src="images/degree.png">
        <p class="has-text-centered">
          Figure 3: <b>Performance of AutoAct based on different degrees of labor division.</b>
          <b><i>One</i></b> is training a single model with all the differentiated data.
          <b><i>Three</i></b> represents the differentiation into three agents: plan, tool, and reflect.
          <b><i>Tool Specified</i></b> indicates further differentiating the tool-agent with one tool, one agent.
        </p>
        <br>
        <img id="model" width="50%" src="images/human.png">
        <p class="has-text-centered">
          Figure 4: <b>Human evaluation of trajectories</b> generated by Llama-2-70b-chat on HotpotQA.
          We compare the number of planning rounds, the logical correctness of thoughts, action types, action parameters, and the overall coherence of each trajectory.
        </p>
        <br>
        <img id="model" width="80%" src="images/case.png">
        <p class="has-text-centered">
          Figure 5: <b>Case study.</b>
          AutoAct (b) successfully addresses the failure in ReAct (a) by employing a more scientific combination of tools and making more accurate tool invocations.
          With more planning rounds, AutoAct (c) can validate its inner answers by continuing more rounds of self-verification.
          While this can also lead to a longer context, gradually deviating AutoAct (d) from the original question.
        </p>
      </div>
    </div>
    <!-- Paper Analysis. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{qiao2024autoact,
  author       = {Shuofei Qiao and Ningyu Zhang and Runnan Fang and Yujie Luo and Wangchunshu Zhou and Yuchen Eleanor Jiang and Chengfei Lv and Huajun Chen},
  title        = {AutoAct: Automatic Agent Learning from Scratch via Self-Planning},
  journal      = {CoRR},
  year         = {2024},
  eprinttype   = {arXiv},
  eprint       = {2401.05268},
}
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


<script>
  $(".grid_item").hover(function () {
    $(this).css("background", "#f2f1f1");
    }, 
    function () {
        $(this).css("background", "#FFFFFF"); 
    });

  // Get the modal element
  // var modal = document.getElementById("myModal");
  var overlay = document.getElementById("overlay");
  var span = document.getElementsByClassName("close")[0];


  // Get the image element and the close button element
  //  // display the GIF as it is
  // var img = document.getElementById("modalImg");
  // var img = document.getElementById("overlayImg");
  // Add event listeners to each GIF element
  var gifs = document.getElementsByClassName("mygif");
  for (var i = 0; i < gifs.length; i++) {
  gifs[i].addEventListener("click", function() {
      //  // display the GIF as it is
      // // Set the modal image source and display the modal
      // img.src = this.src;

      // display the GIF as a new image, will play from the begining
      var img = document.createElement("img");
      img.src = this.src.replace(".png", ".gif");

      // Add the img element to the overlay content and display the overlay
      document.getElementById("overlayContent").appendChild(img);
      

      // modal.style.display = "block";
      overlay.style.display = "block";

      // Hide the body overflow
              document.body.style.overflow = "hidden";
  });
  }

  // Add event listener to close button
  span.addEventListener("click", function() {
  // Remove the img element from the overlay content, hide the overlay, and restore the body overflow
          document.getElementById("overlayContent").innerHTML = "";

  // Hide the modal
  // modal.style.display = "none";
  overlay.style.display = "none";
  document.body.style.overflow = "auto";
  });
</script>
</body>
</html>
